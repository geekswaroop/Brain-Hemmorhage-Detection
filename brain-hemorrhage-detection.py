# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lCJmQMlx1A95sodfbyMjp7bnOhVxM7Hq
"""

from google.colab import drive
drive.mount('/content/drive')

cd /content/drive/

ls

cd My Drive/tumour

ls

import numpy as np
import pandas as pd 
from keras.preprocessing.image import ImageDataGenerator, load_img
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import random
import os
import keras
print(os.listdir("../tumour/train_data/"))

FAST_RUN = False
IMAGE_WIDTH=256
IMAGE_HEIGHT=256
IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)
IMAGE_CHANNELS=3
batch_size=5
filenames=os.listdir("../tumour/train_data/")

sample = random.choice(filenames)
image = load_img("../tumour/train_data/"+sample)
plt.imshow(image)

import pandas as pd
df = pd.read_csv("../tumour/train_labels.csv",header=None,names=["category"])
image_filenames=["Train_{}.png".format(i) for i in range(150)]
#print(image_filenames)
df['category'] = df['category'].replace({0:'not_hem', 1:'hem'})

df['filename']=image_filenames
df

train_df, validate_df = train_test_split(df, test_size=0.20, random_state=42)
train_df = train_df.reset_index(drop=True)

validate_df = validate_df.reset_index(drop=True)

from keras.models import Sequential
from keras import layers
from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation,GlobalMaxPooling2D
from keras import applications
from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers
from keras.applications import VGG16
from keras.models import Model

image_size = 256
input_shape = (image_size, image_size, 3)

epochs = 5
batch_size = 16
pre_trained_model = ResNet50(input_shape=input_shape, include_top=False, weights="imagenet")

from keras.models import Sequential
from keras import layers
from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation,GlobalMaxPooling2D
from keras import applications
from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers
from keras.applications import VGG16
from keras.models import Model

image_size = 256
input_shape = (image_size, image_size, 3)

epochs = 5
batch_size = 16

pre_trained_model = VGG16(input_shape=input_shape, include_top=False, weights="imagenet")
for layer in pre_trained_model.layers[:15]:
    layer.trainable = False

for layer in pre_trained_model.layers[15:]:
    layer.trainable = True
    
last_layer = pre_trained_model.get_layer('block5_pool')
last_output = last_layer.output
    
# Flatten the output layer to 1 dimension
x = GlobalMaxPooling2D()(last_output)
# Add a fully connected layer with 512 hidden units and ReLU activation
x = Dense(1024, activation='relu')(x)
# Add a dropout rate of 0.5
x = Dropout(0.5)(x)
x = Dense(512, activation='relu')(x)
# Add a dropout rate of 0.5
x = Dropout(0.5)(x)
# Add a final sigmoid layer for classification
x = layers.Dense(2, activation='softmax')(x)

model = Model(pre_trained_model.input, x)

model.compile(loss='categorical_crossentropy',
              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),
              metrics=['accuracy'])

model.summary()

from keras.callbacks import EarlyStopping, ReduceLROnPlateau
from keras.callbacks import ModelCheckpoint

earlystop = EarlyStopping(patience=30)
learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', 
                                            patience=2, 
                                            verbose=1, 
                                            factor=0.5, 
                                            min_lr=0.00001)
checkpoint = ModelCheckpoint("best_model.h5", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')
callbacks_list = [checkpoint]
callbacks = [earlystop, learning_rate_reduction,checkpoint]

train_datagen = ImageDataGenerator(
    rotation_range=20,
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    width_shift_range=0.2,
    height_shift_range=0.2
)

train_generator = train_datagen.flow_from_dataframe(
    train_df, 
    "../tumour/train_data", 
    x_col='filename',
    y_col='category',
    target_size=IMAGE_SIZE,
    class_mode='categorical',
    batch_size=batch_size
)

#df = pd.read_csv("../input/train-csv/train_labels.csv",header=None,names=["category"])
#df.iloc[1,0]

validation_datagen = ImageDataGenerator(rescale=1./255)
validation_generator = validation_datagen.flow_from_dataframe(
    validate_df, 
    "../tumour/train_data", 
    x_col='filename',
    y_col='category',
    target_size=IMAGE_SIZE,
    class_mode='categorical',
    batch_size=batch_size
)
total_validate=np.size(validate_df)/2
total_train=np.size(train_df)/2

epochs=3 if FAST_RUN else 100
history = model.fit_generator(
    train_generator, 
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=total_validate//batch_size,
    steps_per_epoch=total_train//batch_size,
    callbacks=callbacks
)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np
import pandas as pd
import cv2
import math
from glob import glob
import os

master = pd.read_csv("labels.csv")
master

#img_path = "../input/train/"

y = []
file_paths = []
for i in range(len(master)):
    file_paths.append(str(master.ix[i][0]) +'.jpg' )
    y.append(master.ix[i][1])
y = np.array(y)

file_paths

train_images = []
for file_path in file_paths:
    #read image
    img = (cv2.imread(file_path))

print(img)

DATADIR = r"images"

X_train=np.array([])
count1=0
for img in os.listdir(DATADIR):
        img_array=cv2.imread(os.path.join(DATADIR,img),cv2.IMREAD_COLOR)
        count1=count1+1
        print(count1)

np.shape(img_array)

